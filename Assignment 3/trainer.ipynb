{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from keras import models\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import deque\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import h5py\n",
    "import copy\n",
    "from state import State_2, UltimateTTT_Move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Q = {}  # state-action values\n",
    "Nsa = {}  # number of times certain state-action pair has been visited\n",
    "Ns = {}   # number of times state has been visited\n",
    "W = {}  # number of total points collected after taking state action pair\n",
    "P = {}  # initial predicted probabilities of taking certain actions in state\n",
    "train_episodes = 100\n",
    "mcts_search = 5\n",
    "n_pit_network = 3\n",
    "playgames_before_training = 2\n",
    "cpuct = 4\n",
    "training_epochs = 2\n",
    "learning_rate = 0.001\n",
    "save_model_path = 'training_dir'\n",
    "def get_empty_board():\n",
    "    return State_2()\n",
    "\n",
    "def print_board(state):\n",
    "    firstRow = \"\"\n",
    "    secondRow = \"\"\n",
    "    thirdRow = \"\"\n",
    "\n",
    "    totalBoard = state.blocks\n",
    "    # Takes each board, saves the rows in a variable, then prints the variables\n",
    "    for boardIndex in range(len(totalBoard)):\n",
    "        firstRow = firstRow + \"|\" + \" \".join(map(lambda x: 'X' if x == 1.0 else ('O' if x == -1.0 else ' '), totalBoard[boardIndex][0])) + \"|\"\n",
    "        secondRow = secondRow + \"|\" + \" \".join(map(lambda x: 'X' if x == 1.0 else ('O' if x == -1.0 else ' '), totalBoard[boardIndex][1])) + \"|\"\n",
    "        thirdRow = thirdRow + \"|\" + \" \".join(map(lambda x: 'X' if x == 1.0 else ('O' if x == -1.0 else ' '), totalBoard[boardIndex][2])) + \"|\"\n",
    "\n",
    "        # if 3 boards have been collected, then it prints the boards out and resets the variables (firstRow, secondRow, etc.)\n",
    "        if boardIndex > 1 and (boardIndex + 1) % 3 == 0:\n",
    "            print(firstRow)\n",
    "            print(secondRow)\n",
    "            print(thirdRow)\n",
    "            print(\"---------------------\")\n",
    "            firstRow = \"\"\n",
    "            secondRow = \"\"\n",
    "            thirdRow = \"\"\n",
    "\n",
    "def possiblePos(state, subBoard):\n",
    "    valid_moves = state.get_valid_moves\n",
    "    if subBoard == 9:\n",
    "        return range(81)\n",
    "    return [(move.index_local_board * 9) + (move.x * 3) + move.y for move in valid_moves]\n",
    "\n",
    "def move(state, action, player):\n",
    "    index_local_board = action // 9\n",
    "    x = (action % 9) // 3\n",
    "    y = action % 3\n",
    "    move = UltimateTTT_Move(index_local_board, x, y, player)\n",
    "    state.act_move(move)\n",
    "    newsubBoard = (x * 3) + y\n",
    "    return state, newsubBoard, state.game_over\n",
    "\n",
    "def fill_winning_boards(state):\n",
    "    new_board = copy.deepcopy(state)\n",
    "    for i in range(9):\n",
    "        if state.global_cells[i] == 1:\n",
    "            new_board.blocks[i] = np.ones((3, 3))\n",
    "        elif state.global_cells[i] == -1:\n",
    "            new_board.blocks[i] = -np.ones((3, 3))\n",
    "    return new_board\n",
    "\n",
    "def board_to_array(state):\n",
    "    board = fill_winning_boards(state)\n",
    "    array = np.zeros((9, 9))\n",
    "    for i in range(9):\n",
    "        for x in range(3):\n",
    "            for y in range(3):\n",
    "                array[i // 3 * 3 + x, i % 3 * 3 + y] = board.blocks[i][x, y]\n",
    "    return array\n",
    "\n",
    "def mcts(s, current_player, mini_board):\n",
    "    if mini_board == 9:\n",
    "        possibleA = range(81)\n",
    "    else:\n",
    "        possibleA = possiblePos(s, mini_board)\n",
    "\n",
    "    sArray = board_to_array(s)\n",
    "    sTuple = tuple(map(tuple, sArray))\n",
    "\n",
    "    # Khởi tạo các dictionary nếu chưa tồn tại\n",
    "    if sTuple not in P:\n",
    "        P[sTuple] = np.ones(81) / 81  # Giá trị khởi tạo policy (cân bằng)\n",
    "    if sTuple not in Ns:\n",
    "        Ns[sTuple] = 0\n",
    "    for a in range(81):  # Khởi tạo tất cả các action\n",
    "        if (sTuple, a) not in Q:\n",
    "            Q[(sTuple, a)] = 0\n",
    "        if (sTuple, a) not in Nsa:\n",
    "            Nsa[(sTuple, a)] = 0\n",
    "        if (sTuple, a) not in W:\n",
    "            W[(sTuple, a)] = 0\n",
    "\n",
    "    if len(possibleA) > 0:\n",
    "        if Ns[sTuple] == 0:  # Tình trạng chưa được mở rộng\n",
    "            policy, v = nn.predict(sArray.reshape(1, 9, 9))\n",
    "            v = v[0][0]\n",
    "            valids = np.zeros(81)\n",
    "            np.put(valids, possibleA, 1)\n",
    "            policy = policy.reshape(81) * valids\n",
    "            policy = policy / np.sum(policy)\n",
    "            P[sTuple] = policy\n",
    "\n",
    "            Ns[sTuple] = 1\n",
    "            return -v\n",
    "\n",
    "        best_uct = -100\n",
    "        for a in possibleA:\n",
    "            uct_a = Q[(sTuple, a)] + cpuct * P[sTuple][a] * (math.sqrt(Ns[sTuple]) / (1 + Nsa[(sTuple, a)]))\n",
    "            if uct_a > best_uct:\n",
    "                best_uct = uct_a\n",
    "                best_a = a\n",
    "\n",
    "        next_state, mini_board, wonBoard = move(s, best_a, current_player)\n",
    "\n",
    "        if wonBoard:\n",
    "            v = 1\n",
    "        else:\n",
    "            current_player *= -1\n",
    "            v = mcts(next_state, current_player, mini_board)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    W[(sTuple, best_a)] += v\n",
    "    Ns[sTuple] += 1\n",
    "    Nsa[(sTuple, best_a)] += 1\n",
    "    Q[(sTuple, best_a)] = W[(sTuple, best_a)] / Nsa[(sTuple, best_a)]\n",
    "    return -v\n",
    "\n",
    "def get_action_probs(init_board, current_player, mini_board):\n",
    "    sArray = board_to_array(init_board)\n",
    "    sTuple = tuple(map(tuple, sArray))\n",
    "\n",
    "    # Khởi tạo các dictionary nếu chưa tồn tại\n",
    "    if sTuple not in P:\n",
    "        P[sTuple] = np.ones(81) / 81  # Giá trị khởi tạo policy (cân bằng)\n",
    "    if sTuple not in Ns:\n",
    "        Ns[sTuple] = 0\n",
    "    for a in range(81):  # Khởi tạo tất cả các action\n",
    "        if (sTuple, a) not in Q:\n",
    "            Q[(sTuple, a)] = 0\n",
    "        if (sTuple, a) not in Nsa:\n",
    "            Nsa[(sTuple, a)] = 0\n",
    "\n",
    "    # Chạy MCTS nhiều lần để lấy xác suất hành động\n",
    "    for _ in range(mcts_search):\n",
    "        s = copy.deepcopy(init_board)\n",
    "        value = mcts(s, current_player, mini_board)\n",
    "\n",
    "    print(\"done one iteration of MCTS\")\n",
    "\n",
    "    actions_dict = {}\n",
    "    for a in possiblePos(init_board, mini_board):\n",
    "        actions_dict[a] = Nsa[(sTuple, a)] / Ns[sTuple] if Ns[sTuple] > 0 else 0\n",
    "\n",
    "    action_probs = np.zeros(81)\n",
    "    for a in actions_dict:\n",
    "        np.put(action_probs, a, actions_dict[a], mode='raise')\n",
    "\n",
    "    return action_probs\n",
    "\n",
    "# def mcts(s, current_player, mini_board):\n",
    "#     if mini_board == 9:\n",
    "#         possibleA = range(81)\n",
    "#     else:\n",
    "#         possibleA = possiblePos(s, mini_board)\n",
    "\n",
    "#     sArray = board_to_array(s)\n",
    "#     sTuple = tuple(map(tuple, sArray))\n",
    "\n",
    "#     if len(possibleA) > 0:\n",
    "#         if sTuple not in P.keys():\n",
    "#             policy, v = nn.predict(sArray.reshape(1, 9, 9))\n",
    "#             v = v[0][0]\n",
    "#             valids = np.zeros(81)\n",
    "#             np.put(valids, possibleA, 1)\n",
    "#             policy = policy.reshape(81) * valids\n",
    "#             policy = policy / np.sum(policy)\n",
    "#             P[sTuple] = policy\n",
    "\n",
    "#             Ns[sTuple] = 1\n",
    "\n",
    "#             for a in possibleA:\n",
    "#                 Q[(sTuple, a)] = 0\n",
    "#                 Nsa[(sTuple, a)] = 0\n",
    "#                 W[(sTuple, a)] = 0\n",
    "#             return -v\n",
    "\n",
    "#         best_uct = -100\n",
    "#         for a in possibleA:\n",
    "#             uct_a = Q[(sTuple, a)] + cpuct * P[sTuple][a] * (math.sqrt(Ns[sTuple]) / (1 + Nsa[(sTuple, a)]))\n",
    "#             if uct_a > best_uct:\n",
    "#                 best_uct = uct_a\n",
    "#                 best_a = a\n",
    "\n",
    "#         next_state, mini_board, wonBoard = move(s, best_a, current_player)\n",
    "\n",
    "#         if wonBoard:\n",
    "#             v = 1\n",
    "#         else:\n",
    "#             current_player *= -1\n",
    "#             v = mcts(next_state, current_player, mini_board)\n",
    "#     else:\n",
    "#         return 0\n",
    "\n",
    "#     W[(sTuple, best_a)] += v\n",
    "#     Ns[sTuple] += 1\n",
    "#     Nsa[(sTuple, best_a)] += 1\n",
    "#     Q[(sTuple, best_a)] = W[(sTuple, best_a)] / Nsa[(sTuple, best_a)]\n",
    "#     return -v\n",
    "\n",
    "# def get_action_probs(init_board, current_player, mini_board):\n",
    "#     for _ in range(mcts_search):\n",
    "#         s = copy.deepcopy(init_board)\n",
    "#         value = mcts(s, current_player, mini_board)\n",
    "\n",
    "#     print(\"done one iteration of MCTS\")\n",
    "\n",
    "#     actions_dict = {}\n",
    "\n",
    "#     sArray = board_to_array(init_board)\n",
    "#     sTuple = tuple(map(tuple, sArray))\n",
    "\n",
    "#     for a in possiblePos(init_board, mini_board):\n",
    "#         actions_dict[a] = Nsa[(sTuple, a)] / Ns[sTuple]\n",
    "\n",
    "#     action_probs = np.zeros(81)\n",
    "\n",
    "#     for a in actions_dict:\n",
    "#         np.put(action_probs, a, actions_dict[a], mode='raise')\n",
    "\n",
    "#     return action_probs\n",
    "\n",
    "def playgame():\n",
    "    done = False\n",
    "    current_player = 1\n",
    "    game_mem = []\n",
    "    mini_board = 9\n",
    "\n",
    "    real_board = get_empty_board()\n",
    "\n",
    "    while not done:\n",
    "        s = copy.deepcopy(real_board)\n",
    "        policy = get_action_probs(s, current_player, mini_board)\n",
    "        policy = policy / np.sum(policy)\n",
    "        game_mem.append([board_to_array(real_board), current_player, policy, None])\n",
    "        action = np.random.choice(len(policy), p=policy)\n",
    "\n",
    "        print(\"policy \", policy)\n",
    "        print(\"chosen action\", action)\n",
    "        print(\"mini-board\", mini_board)\n",
    "        print_board(real_board)\n",
    "\n",
    "        next_state, mini_board, wonBoard = move(real_board, action, current_player)\n",
    "\n",
    "        if len(possiblePos(next_state, mini_board)) == 0:\n",
    "            for tup in game_mem:\n",
    "                tup[3] = 0\n",
    "            return game_mem\n",
    "\n",
    "        if wonBoard:\n",
    "            for tup in game_mem:\n",
    "                if tup[1] == current_player:\n",
    "                    tup[3] = 1\n",
    "                else:\n",
    "                    tup[3] = -1\n",
    "            return game_mem\n",
    "\n",
    "        current_player *= -1\n",
    "        s = next_state\n",
    "\n",
    "def neural_network():\n",
    "    input_layer = layers.Input(shape=(9, 9), name=\"BoardInput\")\n",
    "    reshape = layers.Reshape((9, 9, 1))(input_layer)\n",
    "    conv_1 = layers.Conv2D(128, (3, 3), padding='valid', activation='relu', name='conv1')(reshape)\n",
    "    conv_2 = layers.Conv2D(128, (3, 3), padding='valid', activation='relu', name='conv2')(conv_1)\n",
    "    conv_3 = layers.Conv2D(128, (3, 3), padding='valid', activation='relu', name='conv3')(conv_2)\n",
    "\n",
    "    conv_3_flat = layers.Flatten()(conv_3)\n",
    "\n",
    "    dense_1 = layers.Dense(512, activation='relu', name='dense1')(conv_3_flat)\n",
    "    dense_2 = layers.Dense(256, activation='relu', name='dense2')(dense_1)\n",
    "\n",
    "    pi = layers.Dense(81, activation=\"softmax\", name='pi')(dense_2)\n",
    "    v = layers.Dense(1, activation=\"tanh\", name='value')(dense_2)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=[pi, v])\n",
    "    model.compile(loss=['categorical_crossentropy', 'mean_squared_error'], optimizer=Adam(learning_rate))\n",
    "\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def train_nn(nn, game_mem):\n",
    "    print(\"Training Network\")\n",
    "    print(\"length of game_mem\", len(game_mem))\n",
    "\n",
    "    state = []\n",
    "    policy = []\n",
    "    value = []\n",
    "\n",
    "    for mem in game_mem:\n",
    "        state.append(mem[0])\n",
    "        policy.append(mem[2])\n",
    "        value.append(mem[3])\n",
    "\n",
    "    state = np.array(state)\n",
    "    policy = np.array(policy)\n",
    "    value = np.array(value)\n",
    "\n",
    "    history = nn.fit(state, [policy, value], batch_size=32, epochs=training_epochs, verbose=1)\n",
    "\n",
    "def pit(nn, new_nn):\n",
    "    print(\"Pitting networks\")\n",
    "    nn_wins = 0\n",
    "    new_nn_wins = 0\n",
    "\n",
    "    for _ in range(n_pit_network):\n",
    "        s = get_empty_board()\n",
    "        mini_board = 9\n",
    "\n",
    "        while True:\n",
    "            policy, v = nn.predict(board_to_array(s).reshape(1, 9, 9))\n",
    "            valids = np.zeros(81)\n",
    "\n",
    "            possibleA = possiblePos(s, mini_board)\n",
    "\n",
    "            if len(possibleA) == 0:\n",
    "                break\n",
    "\n",
    "            np.put(valids, possibleA, 1)\n",
    "            policy = policy.reshape(81) * valids\n",
    "            policy = policy / np.sum(policy)\n",
    "            action = np.argmax(policy)\n",
    "\n",
    "            next_state, mini_board, win = move(s, action, 1)\n",
    "            s = next_state\n",
    "\n",
    "            if win:\n",
    "                nn_wins += 1\n",
    "                break\n",
    "\n",
    "            policy, v = new_nn.predict(board_to_array(s).reshape(1, 9, 9))\n",
    "            valids = np.zeros(81)\n",
    "\n",
    "            possibleA = possiblePos(s, mini_board)\n",
    "\n",
    "            if len(possibleA) == 0:\n",
    "                break\n",
    "\n",
    "            np.put(valids, possibleA, 1)\n",
    "            policy = policy.reshape(81) * valids\n",
    "            policy = policy / np.sum(policy)\n",
    "            action = np.argmax(policy)\n",
    "\n",
    "            next_state, mini_board, win = move(s, action, -1)\n",
    "            s = next_state\n",
    "\n",
    "            if win:\n",
    "                new_nn_wins += 1\n",
    "                break\n",
    "\n",
    "    if (new_nn_wins + nn_wins) == 0:\n",
    "        print(\"The game was a complete tie\")\n",
    "        now = datetime.utcnow()\n",
    "        filename = 'tictactoeTie{}.h5'.format(now)\n",
    "        model_path = os.path.join(save_model_path, filename)\n",
    "        nn.save(model_path)\n",
    "        return False\n",
    "\n",
    "    win_percent = float(new_nn_wins) / float(new_nn_wins + nn_wins)\n",
    "    if win_percent > 0.52:\n",
    "        print(\"The new network won\")\n",
    "        print(win_percent)\n",
    "        return True\n",
    "    else:\n",
    "        print(\"The new network lost\")\n",
    "        print(new_nn_wins)\n",
    "        return False\n",
    "output_dir = '/kaggle/working'\n",
    "nn = neural_network()\n",
    "\n",
    "# def train():\n",
    "#     global nn\n",
    "#     global Q\n",
    "#     global Nsa\n",
    "#     global Ns\n",
    "#     global W\n",
    "#     global P\n",
    "\n",
    "#     game_mem = []\n",
    "\n",
    "#     for episode in range(train_episodes):\n",
    "#         nn.save('temp.h5')\n",
    "#         old_nn = models.load_model('temp.h5')\n",
    "\n",
    "#         for _ in range(playgames_before_training):\n",
    "#             game_mem += playgame()\n",
    "\n",
    "#         train_nn(nn, game_mem)\n",
    "#         game_mem = []\n",
    "#         if pit(old_nn, nn):\n",
    "#             del old_nn\n",
    "#             Q = {}\n",
    "#             Nsa = {}\n",
    "#             Ns = {}\n",
    "#             W = {}\n",
    "#             P = {}\n",
    "#         else:\n",
    "#             nn = old_nn\n",
    "#             del old_nn\n",
    "\n",
    "#     now = datetime.utcnow()\n",
    "#     filename = 'tictactoe_MCTS200{}.h5'.format(now)\n",
    "#     model_path = os.path.join(save_model_path, filename)\n",
    "#     nn.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "def save_model(nn, epoch):\n",
    "    \"\"\"\n",
    "    Lưu mô hình sau mỗi epoch\n",
    "    Args:\n",
    "        nn: Mô hình Keras hiện tại\n",
    "        epoch: số epoch hiện tại\n",
    "    \"\"\"\n",
    "    now = datetime.datetime.utcnow()\n",
    "    filename = f'tictactoe_model_epoch_{epoch}_{now.strftime(\"%Y%m%d%H%M%S\")}.h5'\n",
    "    model_path = os.path.join(output_dir, filename)\n",
    "    nn.save(model_path)\n",
    "    print(f'Model saved to {model_path}')\n",
    "    \n",
    "\n",
    "def load_previous_model(output_dir, learning_rate):\n",
    "    \"\"\"\n",
    "    Tải mô hình đã lưu (nếu có), và tái biên dịch với optimizer mới.\n",
    "    \"\"\"\n",
    "    predefined_model_path = '/kaggle/input/ver3/keras/default/1/tictactoe_MCTS200_20241227214414.h5'\n",
    "    try:\n",
    "        # Kiểm tra thư mục /kaggle/working/training_dir trước\n",
    "        if os.path.exists(output_dir):\n",
    "            model_files = [f for f in os.listdir(output_dir) if f.endswith('.h5')]\n",
    "            if model_files:\n",
    "                latest_model = max(model_files, key=lambda f: os.path.getctime(os.path.join(output_dir, f)))\n",
    "                model_path = os.path.join(output_dir, latest_model)\n",
    "                print(f'Loading model from {model_path}')\n",
    "                model = load_model(model_path)\n",
    "    \n",
    "                # Tạo lại optimizer và biên dịch lại mô hình\n",
    "                optimizer = Adam(learning_rate=learning_rate)\n",
    "                model.compile(optimizer=optimizer, loss=['categorical_crossentropy', 'mean_squared_error'])\n",
    "                return model\n",
    "\n",
    "        # Nếu không có mô hình trong output_dir, kiểm tra file cố định trong input\n",
    "        if os.path.exists(predefined_model_path):\n",
    "            print(f'Loading model from {predefined_model_path}')\n",
    "            model = load_model(predefined_model_path)\n",
    "\n",
    "            # Tạo lại optimizer và biên dịch lại mô hình\n",
    "            optimizer = Adam(learning_rate=learning_rate)\n",
    "            model.compile(optimizer=optimizer, loss=['categorical_crossentropy', 'mean_squared_error'])\n",
    "            return model\n",
    "\n",
    "        # Nếu không tìm thấy mô hình ở bất kỳ đâu, trả về None\n",
    "        print('No model found in both training_dir and input. Starting from scratch.')\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f'Error loading model: {e}')\n",
    "        return None\n",
    "# Hàm huấn luyện với cơ chế lưu mô hình\n",
    "# def train():\n",
    "#     global nn\n",
    "#     global Q\n",
    "#     global Nsa\n",
    "#     global Ns\n",
    "#     global W\n",
    "#     global P\n",
    "\n",
    "#     game_mem = []\n",
    "\n",
    "#     # Tải mô hình đã lưu nếu có, nếu không tạo mô hình mới\n",
    "#     nn = load_previous_model(output_dir, learning_rate) if load_previous_model(output_dir, learning_rate) is not None else neural_network()\n",
    "\n",
    "#     for episode in range(train_episodes):\n",
    "        \n",
    "#         # Lưu mô hình sau mỗi epoch\n",
    "#         if episode % 1 == 0:  # Bạn có thể thay đổi tần suất lưu mô hình\n",
    "#             save_model(nn, episode)\n",
    "\n",
    "#         nn.save('temp.h5')\n",
    "#         old_nn = load_model('temp.h5')  # Sử dụng load_model để tải lại mô hình\n",
    "\n",
    "#         # Tạo optimizer mới cho mô hình tải lại\n",
    "#         optimizer = Adam(learning_rate=learning_rate)\n",
    "#         old_nn.compile(optimizer=optimizer, loss=['categorical_crossentropy', 'mean_squared_error'])\n",
    "\n",
    "#         for _ in range(playgames_before_training):\n",
    "#             game_mem += playgame()\n",
    "\n",
    "#         train_nn(nn, game_mem)\n",
    "#         game_mem = []\n",
    "        \n",
    "#         if pit(old_nn, nn):\n",
    "#             del old_nn\n",
    "#             Q = {}\n",
    "#             Nsa = {}\n",
    "#             Ns = {}\n",
    "#             W = {}\n",
    "#             P = {}\n",
    "#         else:\n",
    "#             nn = old_nn\n",
    "#             del old_nn\n",
    "\n",
    "#     # Lưu mô hình cuối cùng\n",
    "#     now = datetime.datetime.utcnow()\n",
    "#     filename = f'tictactoe_MCTS200_{now.strftime(\"%Y%m%d%H%M%S\")}.h5'\n",
    "#     model_path = os.path.join(save_model_path, filename)\n",
    "#     nn.save(model_path)\n",
    "#     print(f'Final model saved to {model_path}')\n",
    "\n",
    "def train():\n",
    "    global nn\n",
    "    global Q\n",
    "    global Nsa\n",
    "    global Ns\n",
    "    global W\n",
    "    global P\n",
    "\n",
    "    game_mem = []\n",
    "    # output_dir = '/kaggle/working/training_dir'\n",
    "    output_dir = '/kaggle/working'\n",
    "    # save_model_path = '/kaggle/working/training_dir'\n",
    "    # Tải mô hình đã lưu nếu có, nếu không tạo mô hình mới\n",
    "    nn = load_previous_model(output_dir, learning_rate) if load_previous_model(output_dir, learning_rate) is not None else neural_network()\n",
    "\n",
    "    for episode in range(train_episodes):\n",
    "        \n",
    "        # Lưu mô hình sau mỗi epoch\n",
    "        if episode % 1 == 0:  # Bạn có thể thay đổi tần suất lưu mô hình\n",
    "            save_model(nn, episode)\n",
    "\n",
    "        for _ in range(playgames_before_training):\n",
    "            game_mem += playgame()\n",
    "\n",
    "        train_nn(nn, game_mem)\n",
    "        game_mem = []\n",
    "\n",
    "        if episode % 10 == 0:  # Ví dụ chỉ thay đổi mô hình sau mỗi 10 epoch\n",
    "            save_model(nn, episode)\n",
    "    \n",
    "    # Lưu mô hình cuối cùng\n",
    "    now = datetime.datetime.utcnow()\n",
    "    filename = f'tictactoe_MCTS200_{now.strftime(\"%Y%m%d%H%M%S\")}.h5'\n",
    "    model_path = os.path.join(save_model_path, filename)\n",
    "    nn.save(model_path)\n",
    "    print(f'Final model saved to {model_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
